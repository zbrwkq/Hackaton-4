{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":38154,"status":"ok","timestamp":1717088690599,"user":{"displayName":"Alexis Jeandenans","userId":"04730578412320209975"},"user_tz":-120},"id":"YaHBDd7TdJX3","outputId":"641e6567-00c7-4b76-833c-e120990d0939"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100\n","15/15 [==============================] - 0s 7ms/step - loss: 44002.3789 - val_loss: 30107.2129\n","Epoch 2/100\n","15/15 [==============================] - 0s 2ms/step - loss: 43611.6641 - val_loss: 29784.1875\n","Epoch 3/100\n","15/15 [==============================] - 0s 2ms/step - loss: 43111.0625 - val_loss: 29220.7402\n","Epoch 4/100\n","15/15 [==============================] - 0s 2ms/step - loss: 41972.5352 - val_loss: 28131.8809\n","Epoch 5/100\n","15/15 [==============================] - 0s 2ms/step - loss: 39584.1992 - val_loss: 25944.4551\n","Epoch 6/100\n","15/15 [==============================] - 0s 2ms/step - loss: 35146.1016 - val_loss: 21849.0059\n","Epoch 7/100\n","15/15 [==============================] - 0s 3ms/step - loss: 28417.5234 - val_loss: 15830.3867\n","Epoch 8/100\n","15/15 [==============================] - 0s 2ms/step - loss: 18339.3457 - val_loss: 8556.0469\n","Epoch 9/100\n","15/15 [==============================] - 0s 2ms/step - loss: 7354.4590 - val_loss: 1958.8284\n","Epoch 10/100\n","15/15 [==============================] - 0s 2ms/step - loss: 950.5080 - val_loss: 119.4627\n","Epoch 11/100\n","15/15 [==============================] - 0s 2ms/step - loss: 210.8501 - val_loss: 120.2435\n","Epoch 12/100\n","15/15 [==============================] - 0s 2ms/step - loss: 108.3937 - val_loss: 113.0691\n","Epoch 13/100\n","15/15 [==============================] - 0s 2ms/step - loss: 79.6445 - val_loss: 92.9751\n","Epoch 14/100\n","15/15 [==============================] - 0s 2ms/step - loss: 65.3258 - val_loss: 73.1175\n","Epoch 15/100\n","15/15 [==============================] - 0s 3ms/step - loss: 56.3263 - val_loss: 63.7796\n","Epoch 16/100\n","15/15 [==============================] - 0s 3ms/step - loss: 46.9639 - val_loss: 54.0370\n","Epoch 17/100\n","15/15 [==============================] - 0s 2ms/step - loss: 39.2625 - val_loss: 43.1852\n","Epoch 18/100\n","15/15 [==============================] - 0s 2ms/step - loss: 31.9063 - val_loss: 35.1208\n","Epoch 19/100\n","15/15 [==============================] - 0s 2ms/step - loss: 25.4163 - val_loss: 29.3144\n","Epoch 20/100\n","15/15 [==============================] - 0s 2ms/step - loss: 21.0835 - val_loss: 22.5108\n","Epoch 21/100\n","15/15 [==============================] - 0s 2ms/step - loss: 16.6012 - val_loss: 17.3965\n","Epoch 22/100\n","15/15 [==============================] - 0s 2ms/step - loss: 14.3715 - val_loss: 16.2025\n","Epoch 23/100\n","15/15 [==============================] - 0s 2ms/step - loss: 11.1554 - val_loss: 12.2814\n","Epoch 24/100\n","15/15 [==============================] - 0s 2ms/step - loss: 9.1832 - val_loss: 13.1142\n","Epoch 25/100\n","15/15 [==============================] - 0s 2ms/step - loss: 8.0373 - val_loss: 9.4440\n","Epoch 26/100\n","15/15 [==============================] - 0s 2ms/step - loss: 6.9451 - val_loss: 9.2209\n","Epoch 27/100\n","15/15 [==============================] - 0s 2ms/step - loss: 6.0594 - val_loss: 8.4522\n","Epoch 28/100\n","15/15 [==============================] - 0s 2ms/step - loss: 5.5800 - val_loss: 7.9209\n","Epoch 29/100\n","15/15 [==============================] - 0s 2ms/step - loss: 4.9467 - val_loss: 7.3464\n","Epoch 30/100\n","15/15 [==============================] - 0s 2ms/step - loss: 4.5023 - val_loss: 7.2237\n","Epoch 31/100\n","15/15 [==============================] - 0s 2ms/step - loss: 4.3035 - val_loss: 6.8306\n","Epoch 32/100\n","15/15 [==============================] - 0s 2ms/step - loss: 3.8860 - val_loss: 6.7508\n","Epoch 33/100\n","15/15 [==============================] - 0s 2ms/step - loss: 3.8099 - val_loss: 5.8476\n","Epoch 34/100\n","15/15 [==============================] - 0s 2ms/step - loss: 3.5016 - val_loss: 6.2821\n","Epoch 35/100\n","15/15 [==============================] - 0s 2ms/step - loss: 3.2973 - val_loss: 5.9803\n","Epoch 36/100\n","15/15 [==============================] - 0s 3ms/step - loss: 3.0891 - val_loss: 5.0618\n","Epoch 37/100\n","15/15 [==============================] - 0s 2ms/step - loss: 3.1606 - val_loss: 5.8568\n","Epoch 38/100\n","15/15 [==============================] - 0s 3ms/step - loss: 2.6489 - val_loss: 4.9775\n","Epoch 39/100\n","15/15 [==============================] - 0s 2ms/step - loss: 2.5455 - val_loss: 4.8470\n","Epoch 40/100\n","15/15 [==============================] - 0s 2ms/step - loss: 2.5744 - val_loss: 4.5019\n","Epoch 41/100\n","15/15 [==============================] - 0s 2ms/step - loss: 2.3865 - val_loss: 4.4072\n","Epoch 42/100\n","15/15 [==============================] - 0s 2ms/step - loss: 2.7631 - val_loss: 4.8281\n","Epoch 43/100\n","15/15 [==============================] - 0s 2ms/step - loss: 2.7191 - val_loss: 4.2550\n","Epoch 44/100\n","15/15 [==============================] - 0s 3ms/step - loss: 2.3213 - val_loss: 4.1511\n","Epoch 45/100\n","15/15 [==============================] - 0s 3ms/step - loss: 2.0951 - val_loss: 4.1841\n","Epoch 46/100\n","15/15 [==============================] - 0s 2ms/step - loss: 2.1198 - val_loss: 3.6337\n","Epoch 47/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.7551 - val_loss: 3.8717\n","Epoch 48/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.6381 - val_loss: 3.6811\n","Epoch 49/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.7927 - val_loss: 3.0864\n","Epoch 50/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.9472 - val_loss: 3.6942\n","Epoch 51/100\n","15/15 [==============================] - 0s 2ms/step - loss: 2.0905 - val_loss: 3.2550\n","Epoch 52/100\n","15/15 [==============================] - 0s 4ms/step - loss: 1.3684 - val_loss: 2.7248\n","Epoch 53/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.4859 - val_loss: 3.0130\n","Epoch 54/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.3646 - val_loss: 2.6195\n","Epoch 55/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.2673 - val_loss: 3.1080\n","Epoch 56/100\n","15/15 [==============================] - 0s 2ms/step - loss: 1.2711 - val_loss: 2.4601\n","Epoch 57/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.1863 - val_loss: 2.7800\n","Epoch 58/100\n","15/15 [==============================] - 0s 4ms/step - loss: 1.2453 - val_loss: 2.3634\n","Epoch 59/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0983 - val_loss: 2.5404\n","Epoch 60/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.1917 - val_loss: 2.1534\n","Epoch 61/100\n","15/15 [==============================] - 0s 3ms/step - loss: 0.9717 - val_loss: 2.5251\n","Epoch 62/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0989 - val_loss: 2.1590\n","Epoch 63/100\n","15/15 [==============================] - 0s 5ms/step - loss: 0.9135 - val_loss: 2.2455\n","Epoch 64/100\n","15/15 [==============================] - 0s 5ms/step - loss: 0.8473 - val_loss: 1.9723\n","Epoch 65/100\n","15/15 [==============================] - 0s 3ms/step - loss: 0.8540 - val_loss: 2.1976\n","Epoch 66/100\n","15/15 [==============================] - 0s 3ms/step - loss: 1.0048 - val_loss: 1.6938\n","Epoch 67/100\n","15/15 [==============================] - 0s 3ms/step - loss: 0.9310 - val_loss: 2.0809\n","Epoch 68/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.8155 - val_loss: 1.7215\n","Epoch 69/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.7422 - val_loss: 1.6958\n","Epoch 70/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.7726 - val_loss: 1.8077\n","Epoch 71/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.8198 - val_loss: 1.4032\n","Epoch 72/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.6969 - val_loss: 1.9212\n","Epoch 73/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.6437 - val_loss: 1.4099\n","Epoch 74/100\n","15/15 [==============================] - 0s 3ms/step - loss: 0.6092 - val_loss: 1.6965\n","Epoch 75/100\n","15/15 [==============================] - 0s 5ms/step - loss: 0.5822 - val_loss: 1.4310\n","Epoch 76/100\n","15/15 [==============================] - 0s 3ms/step - loss: 0.5155 - val_loss: 1.3546\n","Epoch 77/100\n","15/15 [==============================] - 0s 3ms/step - loss: 0.6797 - val_loss: 2.0823\n","Epoch 78/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.7776 - val_loss: 1.2530\n","Epoch 79/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.4742 - val_loss: 1.3972\n","Epoch 80/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.4681 - val_loss: 1.2077\n","Epoch 81/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.4203 - val_loss: 1.3518\n","Epoch 82/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.4776 - val_loss: 1.2502\n","Epoch 83/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.5640 - val_loss: 1.6264\n","Epoch 84/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.5543 - val_loss: 1.0839\n","Epoch 85/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.4914 - val_loss: 0.9777\n","Epoch 86/100\n","15/15 [==============================] - 0s 3ms/step - loss: 0.5389 - val_loss: 1.2993\n","Epoch 87/100\n","15/15 [==============================] - 0s 5ms/step - loss: 0.6380 - val_loss: 0.9559\n","Epoch 88/100\n","15/15 [==============================] - 0s 5ms/step - loss: 0.3859 - val_loss: 0.9104\n","Epoch 89/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.3588 - val_loss: 1.0042\n","Epoch 90/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.3112 - val_loss: 0.8766\n","Epoch 91/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.3145 - val_loss: 0.8665\n","Epoch 92/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.3006 - val_loss: 0.9210\n","Epoch 93/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.3162 - val_loss: 0.9489\n","Epoch 94/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.2714 - val_loss: 0.8472\n","Epoch 95/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.2470 - val_loss: 0.7798\n","Epoch 96/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.2621 - val_loss: 0.8943\n","Epoch 97/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.2441 - val_loss: 0.7733\n","Epoch 98/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.2529 - val_loss: 0.8010\n","Epoch 99/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.2778 - val_loss: 0.8819\n","Epoch 100/100\n","15/15 [==============================] - 0s 2ms/step - loss: 0.2687 - val_loss: 0.7110\n","WARNING:tensorflow:5 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x000001B0A1085000> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n","2/2 [==============================] - 0s 2ms/step\n","RMSE on test data: 2.1822896280562945\n","1/1 [==============================] - 0s 33ms/step\n","  country_name  prediction\n","0       France   29.012457\n","1          USA   97.247726\n","2        China   90.024002\n"]},{"name":"stderr","output_type":"stream","text":["c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_regression.py:492: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.\n","  warnings.warn(\n"]}],"source":["import pandas as pd\n","from sqlalchemy import create_engine\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.losses import MeanSquaredError\n","import joblib\n","\n","# Configuration de la connexion à la base de données\n","engine = create_engine('mysql+pymysql://360556_root:hackathonipssi*@mysql-hackathonipssi.alwaysdata.net:3306/hackathonipssi_mia4')\n","\n","# Lecture des données historiques\n","df = pd.read_sql_table(\"Result\", con=engine)\n","\n","# Transformation des données\n","df['gold'] = (df['medal_type'] == 'GOLD').astype(int)\n","df['silver'] = (df['medal_type'] == 'SILVER').astype(int)\n","df['bronze'] = (df['medal_type'] == 'BRONZE').astype(int)\n","medals_df = df.groupby(\"country_name\").agg({\n","    \"gold\": \"sum\",\n","    \"silver\": \"sum\",\n","    \"bronze\": \"sum\"\n","})\n","medals_df['total'] = medals_df['gold'] + medals_df['silver'] + medals_df['bronze']\n","\n","# Préparation des caractéristiques\n","X = medals_df[['gold', 'silver', 'bronze', 'total']]\n","y = medals_df['total']\n","\n","# Division des données en ensembles d'entraînement et de test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Normalisation des données\n","scaler = StandardScaler()\n","X_train_scaled = scaler.fit_transform(X_train)\n","X_test_scaled = scaler.transform(X_test)\n","\n","# Définir le modèle de réseau de neurones\n","model = Sequential([\n","    Dense(128, input_shape=(X_train_scaled.shape[1],), activation='relu'),\n","    Dense(64, activation='relu'),\n","    Dense(32, activation='relu'),\n","    Dense(1, activation='linear')\n","])\n","\n","# Compiler le modèle avec la perte et l'optimiseur correctement configurés\n","model.compile(optimizer=Adam(learning_rate=0.001), loss=MeanSquaredError())\n","\n","# Entraîner le modèle\n","model.fit(X_train_scaled, y_train, epochs=100, batch_size=10, validation_split=0.2)\n","\n","# Sauvegarde du modèle\n","model.save('medal_predictor_deep_learning.h5')\n","joblib.dump(scaler, 'scaler.h5')\n","\n","# Évaluation du modèle\n","y_pred = model.predict(X_test_scaled)\n","rmse = mean_squared_error(y_test, y_pred, squared=False)\n","print(\"RMSE on test data:\", rmse)\n","\n","# Préparation des données pour 2024\n","new_data_2024 = pd.DataFrame({\n","    \"country_name\": [\"France\", \"USA\", \"China\"],\n","    \"gold\": [10, 35, 40],\n","    \"silver\": [12, 32, 30],\n","    \"bronze\": [9, 30, 20]\n","})\n","new_data_2024['total'] = new_data_2024['gold'] + new_data_2024['silver'] + new_data_2024['bronze']\n","X_new = new_data_2024[['gold', 'silver', 'bronze', 'total']]\n","X_new_scaled = scaler.transform(X_new)\n","\n","# Utilisation du modèle pour faire des prédictions pour 2024\n","predictions_2024 = model.predict(X_new_scaled)\n","new_data_2024['prediction'] = predictions_2024\n","print(new_data_2024[['country_name', 'prediction']])\n"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNHy0+sgUj6O0LXC+OIshrB","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":0}
